twitter {
  auth {
    consumerTokenKey = ${TWITTER_CONSUMER_TOKEN_KEY}
    consumerTokenSecret = ${TWITTER_CONSUMER_TOKEN_SECRET}

    accessTokenKey = ${TWITTER_ACCESS_TOKEN_KEY}
    accessTokenSecret = ${TWITTER_ACCESS_TOKEN_SECRET}
  }

  version = "1.1"

  rest {
    api = "https://api.twitter.com"
    media = "https://upload.twitter.com"
  }

  streaming {
    public = "https://stream.twitter.com"
    user = "https://userstream.twitter.com"
    site = "https://sitestream.twitter.com"
  }
}

akka {

  stdout-loglevel = "INFO"
  loglevel = "INFO"
  extensions = [akka.persistence.Persistence]

  actor {
    default-mailbox.stash-capacity = 10000
  }

  persistence {

    journal {
      plugin = "jdbc-journal"
      auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      plugin = "jdbc-snapshot-store"
      auto-start-snapshot-stores = ["jdbc-snapshot-store"]
    }
  }


  kafka {

    bootstrap-servers = "localhost:9092"

    producer {
      # Tuning parameter of how many sends that can run in parallel.
      parallelism = 100

      # How long to wait for `KafkaProducer.close`
      close-timeout = 60s

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the producer stages. Some blocking may occur.
      # When this value is empty, the dispatcher configured for the stream
      # will be used.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
      # can be defined in this configuration section.
      # kafka-clients {
      # }
    }

    consumer {
      # Tuning property of scheduled polls.
      poll-interval = 50ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that blocking of the thread that
      # is executing the stage will be blocked.
      poll-timeout = 50ms

      # The stage will be await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # How long to wait for `KafkaConsumer.close`
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `TimeoutException`.
      commit-timeout = 15s

      # If the KafkaConsumer can't connect to the broker the poll will be
      # aborted after this timeout. The KafkaConsumerActor will throw
      # org.apache.kafka.common.errors.WakeupException which will be ignored
      # until max-wakeups limit gets exceeded.
      wakeup-timeout = 3s

      # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
      max-wakeups = 10

      wakeup-debug = true

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        # Disable auto-commit by default
        enable.auto.commit = false
      }
    }
  }
}

jdbc-journal {
  slick = ${slick}
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  slick = ${slick}
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  slick = ${slick}
}


slick {
  profile = "slick.jdbc.PostgresProfile$"
  db {
    host = "localhost"
    host = ${?POSTGRES_HOST}
    url = "jdbc:postgresql://127.0.0.1:5432/postgres?reWriteBatchedInserts=true"
    user = "postgres"
    user = ${?POSTGRES_USER}
    password = "postgres"
    password = ${?POSTGRES_PASSWORD}
    driver = "org.postgresql.Driver"
    numThreads = 5
    maxConnections = 5
    minConnections = 1

  }
}